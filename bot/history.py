import os
import json
import pytz
import telebot
import logging
from datetime import datetime
from bs4 import BeautifulSoup
import xml.etree.ElementTree as ET
import re
import sys

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.FileHandler("history.log"),
        logging.StreamHandler()
    ]
)

# –¢–æ–∫–µ–Ω –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
TOKEN = os.getenv("TELEGRAM_HISTORY_TOKEN")
CHANNEL_ID = "@historySvoih"
SEEN_IDS_FILE = "seen_ids1.txt"  # –í –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞
HISTORY_FILE = "public/history.html"
SITEMAP_FILE = "public/sitemap.xml"
RSS_FILE = "public/history_rss.xml"
moscow = pytz.timezone("Europe/Moscow")

# –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç–æ–∫–µ–Ω–∞
if not TOKEN:
    logging.error("TELEGRAM_HISTORY_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è")
    sys.exit(1)

bot = telebot.TeleBot(TOKEN)

# –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –±–æ—Ç–∞
try:
    bot.get_me()
    logging.info("–ë–æ—Ç —É—Å–ø–µ—à–Ω–æ –∞–≤—Ç–æ—Ä–∏–∑–æ–≤–∞–Ω")
except Exception as e:
    logging.error(f"–û—à–∏–±–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ –±–æ—Ç–∞: {e}")
    sys.exit(1)

# === –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ ===

def load_seen_ids():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç ID —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π."""
    logging.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {SEEN_IDS_FILE}")
    if not os.path.exists(SEEN_IDS_FILE):
        logging.info(f"–§–∞–π–ª {SEEN_IDS_FILE} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç, —Å–æ–∑–¥–∞—ë–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
        with open(SEEN_IDS_FILE, "w", encoding="utf-8") as f:
            json.dump([], f)
        return set()
    
    try:
        with open(SEEN_IDS_FILE, "r", encoding="utf-8") as f:
            content = f.read().strip()
            if not content:
                logging.info(f"–§–∞–π–ª {SEEN_IDS_FILE} –ø—É—Å—Ç, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫")
                return set()
            return set(json.loads(content))
    except json.JSONDecodeError as e:
        logging.error(f"–û—à–∏–±–∫–∞ JSON –≤ {SEEN_IDS_FILE}: {e}. –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Ñ–∞–π–ª")
        with open(SEEN_IDS_FILE, "w", encoding="utf-8") as f:
            json.dump([], f)
        return set()
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è {SEEN_IDS_FILE}: {e}")
        return set()

def save_seen_ids(ids):
    """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Å–ø–∏—Å–æ–∫ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π."""
    try:
        with open(SEEN_IDS_FILE, "w", encoding="utf-8") as f:
            json.dump(list(ids), f)
        logging.info(f"–°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(ids)} ID –≤ {SEEN_IDS_FILE}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –∑–∞–ø–∏—Å–∏ –≤ {SEEN_IDS_FILE}: {e}")

def clean_text(text):
    """–û—á–∏—â–∞–µ—Ç —Ç–µ–∫—Å—Ç –æ—Ç –Ω–µ–∂–µ–ª–∞—Ç–µ–ª—å–Ω—ã—Ö —Ñ—Ä–∞–∑."""
    if not text:
        return ""
    unwanted = ["–ü–æ–¥–ø–∏—Å–∞—Ç—å—Å—è –Ω–∞ –∏—Å—Ç–æ—Ä–∏—é –¥–ª—è —Å–≤–æ–∏—Ö", "https://t.me/historySvoih"]
    for phrase in unwanted:
        text = text.replace(phrase, "")
    return text.strip()

def format_post(message):
    """–§–æ—Ä–º–∞—Ç–∏—Ä—É–µ—Ç –ø–æ—Å—Ç –≤ HTML –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –¥–∞–Ω–Ω—ã–µ –¥–ª—è JSON-LD –∏ RSS."""
    timestamp = message.date
    iso_time = datetime.fromtimestamp(timestamp, moscow).strftime("%Y-%m-%dT%H:%M:%S+03:00")
    formatted_time = datetime.fromtimestamp(timestamp, moscow).strftime("%d.%m.%Y %H:%M")
    caption = clean_text(message.caption or "")
    text = clean_text(message.text or "")
    html = "<article class='news-item'>\n"
    json_ld_article = {
        "@type": "NewsArticle",
        "headline": caption[:200] or text[:200] or "–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–µ —Å–æ–±—ã—Ç–∏–µ",
        "description": text[:500] or caption[:500] or "",
        "datePublished": iso_time,
        "author": {
            "@type": "Organization",
            "name": "SVOih History Team"
        },
        "publisher": {
            "@type": "Organization",
            "name": "–ù–æ–≤–æ—Å—Ç–∏ –¥–ª—è –°–≤–æ–∏—Ö",
            "logo": {"@type": "ImageObject", "url": "https://newsforsvoi.ru/logo.png"}
        },
        "url": f"https://t.me/{CHANNEL_ID[1:]}/{message.message_id}"
    }
    file_url = None

    if message.content_type == "photo":
        photos = message.photo
        try:
            file_info = bot.get_file(photos[-1].file_id)
            file_url = f"https://api.telegram.org/file/bot{TOKEN}/{file_info.file_path}"
            html += f"<img src='{file_url}' alt='–§–æ—Ç–æ —Å–æ–±—ã—Ç–∏—è' class='history-image' />\n"
            json_ld_article["image"] = {
                "@type": "ImageObject",
                "url": file_url,
                "width": photos[-1].width,
                "height": photos[-1].height
            }
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ñ–æ—Ç–æ –¥–ª—è –ø–æ—Å—Ç–∞ {message.message_id}: {e}")
            return "", None
    elif message.content_type == "video":
        try:
            size = getattr(message.video, "file_size", 0)
            if size <= 20_000_000:
                file_info = bot.get_file(message.video.file_id)
                file_url = f"https://api.telegram.org/file/bot{TOKEN}/{file_info.file_path}"
                html += f"<video controls src='{file_url}'></video>\n"
                json_ld_article["video"] = {
                    "@type": "VideoObject",
                    "contentUrl": file_url,
                    "uploadDate": iso_time
                }
            else:
                logging.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –≤–∏–¥–µ–æ >20MB –¥–ª—è –ø–æ—Å—Ç–∞ {message.message_id}: {size}")
                return "", None
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–∏–¥–µ–æ –¥–ª—è –ø–æ—Å—Ç–∞ {message.message_id}: {e}")
            return "", None

    if caption:
        html += f"<p><b>{caption}</b></p>\n"
    if text and text != caption:
        html += f"<p>{text}</p>\n"

    html += f"<a href='https://t.me/{CHANNEL_ID[1:]}/{message.message_id}' target='_blank'>–ß–∏—Ç–∞—Ç—å –≤ Telegram</a>\n"
    html += f"<div class='timestamp' data-ts='{iso_time}'>üïí {formatted_time}</div>\n"
    html += "</article>\n"
    return html, json_ld_article

def fetch_latest_posts():
    """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞."""
    try:
        updates = bot.get_updates()
        posts = [
            u.channel_post
            for u in updates
            if u.channel_post and u.channel_post.chat.username == CHANNEL_ID[1:]
        ]
        logging.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(posts)} –ø–æ—Å—Ç–æ–≤ –∏–∑ –∫–∞–Ω–∞–ª–∞ {CHANNEL_ID}")
        return list(reversed(posts[-12:])) if posts else []
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –ø–æ—Å—Ç–æ–≤: {e}")
        return []

def update_sitemap():
    """–û–±–Ω–æ–≤–ª—è–µ—Ç <lastmod> –¥–ª—è history.html –≤ sitemap.xml."""
    now = datetime.now(moscow).strftime("%Y-%m-%dT%H:%M:%S+03:00")
    try:
        tree = ET.parse(SITEMAP_FILE)
        root = tree.getroot()
        for url in root.findall("{http://www.sitemaps.org/schemas/sitemap/0.9}url"):
            loc = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}loc")
            if loc.text == "https://newsforsvoi.ru/history.html":
                lastmod = url.find("{http://www.sitemaps.org/schemas/sitemap/0.9}lastmod")
                lastmod.text = now
                break
        tree.write(SITEMAP_FILE, encoding="utf-8", xml_declaration=True)
        logging.info(f"–û–±–Ω–æ–≤–ª—ë–Ω sitemap.xml –¥–ª—è history.html: {now}")
    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª {SITEMAP_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ bot.py —Å–æ–∑–¥–∞–ª sitemap.xml.")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ sitemap.xml: {e}")

def generate_rss(posts):
    """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç RSS –¥–ª—è history.html."""
    rss_items = ""
    for html_block, json_ld in posts:
        title_match = re.search(r"<p><b>(.*?)</b></p>", html_block) or re.search(r"<p>(.*?)</p>", html_block)
        link_match = re.search(r"<a href='(https://t\.me/[^']+)'", html_block)
        date_match = re.search(r"data-ts='([^']+)'", html_block)

        title = title_match.group(1) if title_match else json_ld["headline"]
        link = link_match.group(1) if link_match else f"https://t.me/{CHANNEL_ID[1:]}"
        pub_date = (
            datetime.strptime(date_match.group(1), "%Y-%m-%dT%H:%M:%S+03:00").strftime("%a, %d %b %Y %H:%M:%S +0300")
            if date_match
            else datetime.now(moscow).strftime("%a, %d %b %Y %H:%M:%S +0300")
        )

        rss_items += f"""
<item>
  <title>{title}</title>
  <link>{link}</link>
  <description>{json_ld["description"]}</description>
  <pubDate>{pub_date}</pubDate>
</item>
"""

    rss = f"""<?xml version="1.0" encoding="UTF-8" ?>
<rss version="2.0">
  <channel>
    <title>–ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –°–≤–æ–∏—Ö</title>
    <link>https://newsforsvoi.ru/history.html</link>
    <description>–ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ Telegram-–∫–∞–Ω–∞–ª–∞ –ò—Å—Ç–æ—Ä–∏—è –¥–ª—è –°–≤–æ–∏—Ö</description>
    {rss_items}
  </channel>
</rss>
"""
    try:
        os.makedirs(os.path.dirname(RSS_FILE), exist_ok=True)
        with open(RSS_FILE, "w", encoding="utf-8") as f:
            f.write(rss)
        logging.info("history_rss.xml –æ–±–Ω–æ–≤–ª—ë–Ω")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ history_rss.xml: {e}")

def update_history_html(html, json_ld_article):
    """–î–æ–±–∞–≤–ª—è–µ—Ç –ø–æ—Å—Ç –∏ –æ–±–Ω–æ–≤–ª—è–µ—Ç JSON-LD –≤ history.html."""
    os.makedirs(os.path.dirname(HISTORY_FILE), exist_ok=True)

    try:
        if os.path.exists(HISTORY_FILE):
            with open(HISTORY_FILE, "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f, "html.parser")
        else:
            logging.warning(f"–§–∞–π–ª {HISTORY_FILE} –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —à–∞–±–ª–æ–Ω history.html")
            with open("history.html", "r", encoding="utf-8") as f:
                soup = BeautifulSoup(f, "html.parser")
    except FileNotFoundError:
        logging.error(f"–§–∞–π–ª history.html –Ω–µ –Ω–∞–π–¥–µ–Ω, —Å–æ–∑–¥–∞–π—Ç–µ —à–∞–±–ª–æ–Ω –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞")
        return
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —á—Ç–µ–Ω–∏–∏ history.html: {e}")
        return

    history_container = soup.find("div", id="history-container")
    if history_container:
        new_article = BeautifulSoup(html, "html.parser")
        history_container.insert(0, new_article)
    else:
        logging.error("–ö–æ–Ω—Ç–µ–π–Ω–µ—Ä #history-container –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ history.html")
        return

    schema_script = soup.find("script", id="schema-org")
    if schema_script and json_ld_article:
        try:
            schema_data = json.loads(schema_script.string)
            schema_data["mainEntity"]["itemListElement"].insert(0, {
                "@type": "ListItem",
                "position": len(schema_data["mainEntity"]["itemListElement"]) + 1,
                "item": json_ld_article
            })
            schema_script.string = json.dumps(schema_data, ensure_ascii=False, indent=2)
        except Exception as e:
            logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ JSON-LD: {e}")

    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            f.write(str(soup))
        logging.info(f"–û–±–Ω–æ–≤–ª—ë–Ω {HISTORY_FILE}")
    except Exception as e:
        logging.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø–∏—Å–∏ {HISTORY_FILE}: {e}")

    if os.path.exists(SITEMAP_FILE):
        update_sitemap()

# === –û—Å–Ω–æ–≤–Ω–∞—è –ª–æ–≥–∏–∫–∞ ===

def process_initial_posts():
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–µ –ø–æ—Å—Ç—ã –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –±–æ—Ç–∞."""
    posts = fetch_latest_posts()
    seen_ids = load_seen_ids()
    new_posts = []

    for post in posts:
        if post.message_id in seen_ids:
            logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç {post.message_id}: —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
            continue

        html, json_ld_article = format_post(post)
        if html and json_ld_article:
            update_history_html(html, json_ld_article)
            new_posts.append((html, json_ld_article))
            seen_ids.add(post.message_id)
            logging.info(f"–î–æ–±–∞–≤–ª–µ–Ω –Ω–∞—á–∞–ª—å–Ω—ã–π –ø–æ—Å—Ç {post.message_id}")
        else:
            logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Å—Ç {post.message_id}")

    if new_posts:
        save_seen_ids(seen_ids)
        generate_rss(new_posts)
        logging.info(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(new_posts)} –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤")
    else:
        logging.info("–ù–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ –Ω–µ—Ç")

def handle_channel_post(message):
    """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –Ω–æ–≤—ã–µ –ø–æ—Å—Ç—ã –∏–∑ –∫–∞–Ω–∞–ª–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏."""
    if message.chat.username != CHANNEL_ID[1:]:
        logging.warning(f"–ü–æ–ª—É—á–µ–Ω –ø–æ—Å—Ç –∏–∑ –¥—Ä—É–≥–æ–≥–æ –∫–∞–Ω–∞–ª–∞: {message.chat.username}")
        return

    seen_ids = load_seen_ids()
    if message.message_id in seen_ids:
        logging.info(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø–æ—Å—Ç {message.message_id}: —É–∂–µ –æ–±—Ä–∞–±–æ—Ç–∞–Ω")
        return

    html, json_ld_article = format_post(message)
    if html and json_ld_article:
        update_history_html(html, json_ld_article)
        seen_ids.add(message.message_id)
        save_seen_ids(seen_ids)
        generate_rss([(html, json_ld_article)])
        logging.info(f"–î–æ–±–∞–≤–ª–µ–Ω –ø–æ—Å—Ç {message.message_id}")
    else:
        logging.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –ø–æ—Å—Ç {message.message_id}")

if __name__ == "__main__":
    logging.info("–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞ –¥–ª—è –∫–∞–Ω–∞–ª–∞ @historySvoih")
    process_initial_posts()
    logging.info("–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã –ø–æ—Å–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–∞—á–∞–ª—å–Ω—ã—Ö –ø–æ—Å—Ç–æ–≤")